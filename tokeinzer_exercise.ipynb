{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the BasicTokenizer class, with the following three core functions:\n",
    "\n",
    "def train(self, text, vocab_size, verbose=False)\n",
    "    * define vocab_size (check again what the base here was)\n",
    "    * define merges (how many convolutions?)\n",
    "    * encode text to byes\n",
    "    * map positions (look up exactly why tis is again)\n",
    "    * find the most common pairs and merge them, assigning them new idx\n",
    "    after vocab\n",
    "    \n",
    "\n",
    "def encode(self, text)\n",
    "def decode(self, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTokenizer():\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        # special tokens etc\n",
    "        self.vocab = \"\" # ?\n",
    "        # where does the vocab_size then come from? The textual input?\n",
    "    \n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {}\n",
    "for i in range(0,256):\n",
    "    vocab_map[i] = chr(i)\n",
    "vocab_map\n",
    "\n",
    "def find_next_vocab_idx(vocab_map):\n",
    "    return len(vocab_map)\n",
    "\n",
    "find_next_vocab_idx(vocab_map)\n",
    "\n",
    "def next_non_terminal_symbol(vocab_map_idx):\n",
    "    symbol_idx = vocab_map_idx - 256\n",
    "    ASCII_Z = 90\n",
    "    return chr(ASCII_Z - symbol_idx) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaabdaaabac\n",
      "ZabdZabac\n",
      "YbdYbac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'XdXac'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"aaabdaaabac\"\n",
    "\n",
    "# find out wich n-gram appears most often for merge - first start with bigram\n",
    "\n",
    "def count_bigrams(text):\n",
    "    bigram_occurence = {}\n",
    "    for i in range(len(text) -1):\n",
    "        bigram = text[i:i+2]\n",
    "        if not bigram in bigram_occurence:\n",
    "            bigram_occurence[bigram] = 0\n",
    "        bigram_occurence[bigram] += 1\n",
    "    return bigram_occurence\n",
    "\n",
    "def find_max_occurence(text):\n",
    "    max_count = 0\n",
    "    max_bigram = None\n",
    "    for bigram, count in count_bigrams(text).items():\n",
    "        #print(bigram, count)\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_bigram = bigram\n",
    "        \n",
    "    return max_bigram\n",
    "\n",
    "def merge(text, vocab_map):\n",
    "    max_bigram = find_max_occurence(text)\n",
    "    next_substitution = next_non_terminal_symbol(find_next_vocab_idx(vocab_map))\n",
    "    text = text.replace(max_bigram, next_substitution)\n",
    "    vocab_map[find_next_vocab_idx(vocab_map)] = max_bigram\n",
    "    return text\n",
    "\n",
    "def train(text, vocab_size, num_merges):\n",
    "    for i in range(num_merges):\n",
    "        print(text)\n",
    "        text = merge(text,vocab_map)\n",
    "    return text\n",
    "\n",
    "# print(merge(text, vocab_map))\n",
    "# print(list(vocab_map.items())[-1])\n",
    "\n",
    "train(text, 256, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-exercises-VqUQgDz3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
