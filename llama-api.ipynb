{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Large Language Model with Ollama (Llama 3 8B)\n",
    "\n",
    "This notebook uses the Llama 3 8B model running locally on your machine. Llama 3 8B offers significantly better performance than the base model while still being resource-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3:8b\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running with the following models:\n",
      " - german-mixtral:latest\n",
      " - mixtral:latest\n",
      " - llama3:8b\n",
      " - mannix/llama3.1-8b-abliterated:latest\n",
      " - llama3.1:latest\n",
      " - dolphin-llama3:latest\n",
      "\n",
      "✓ Llama 3 8B model found\n"
     ]
    }
   ],
   "source": [
    "def check_ollama_status():\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_ENDPOINT}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            if models:\n",
    "                print(f\"Ollama is running with the following models:\")\n",
    "                for model in models:\n",
    "                    print(f\" - {model['name']}\")\n",
    "                \n",
    "                # Check if Llama 3 8B is available\n",
    "                llama_models = [m for m in models if 'llama3:8b' in m['name'].lower()]\n",
    "                if llama_models:\n",
    "                    print(f\"\\n✓ Llama 3 8B model found\")\n",
    "                else:\n",
    "                    print(f\"\\n⚠ Llama 3 8B was not found\")\n",
    "            else:\n",
    "                print(\"Ollama is running, but no models were found. Run 'ollama pull llama3:8b' in the terminal.\")\n",
    "        else:\n",
    "            print(\"Could not connect to Ollama.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        print(\"Make sure the Ollama service is running with: 'systemctl --user start ollama'\")\n",
    "\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3Client:\n",
    "    def __init__(self, base_url: str = OLLAMA_ENDPOINT, model: str = MODEL_NAME):\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        \n",
    "        # Default parameters optimized for llama3\n",
    "        self.default_params = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"num_ctx\": 4096  \n",
    "        }\n",
    "        \n",
    "    def chat_completion(self, messages: List[Dict[str, str]], \n",
    "                         max_tokens: Optional[int] = None, \n",
    "                         stream: bool = False,\n",
    "                         **kwargs):\n",
    "        endpoint = f\"{self.base_url}/api/chat\"\n",
    "        \n",
    "        # For inference\n",
    "        options = self.default_params.copy()\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            options[key] = value\n",
    "            \n",
    "        if max_tokens:\n",
    "            options[\"num_predict\"] = max_tokens\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "            \"options\": options\n",
    "        }\n",
    "            \n",
    "        if stream:\n",
    "            response = requests.post(endpoint, json=payload, stream=True)\n",
    "            return response.iter_lines()\n",
    "        else:\n",
    "            response = requests.post(endpoint, json=payload)\n",
    "            return response.json()\n",
    "    \n",
    "    def get_token_count(self, messages: List[Dict[str, str]]):\n",
    "        \"\"\"Estimate the number of tokens in the messages\"\"\"\n",
    "        # ~4 chars/token for llama3\n",
    "        total_chars = sum(len(m[\"content\"]) for m in messages)\n",
    "        estimated_tokens = total_chars // 4\n",
    "        return estimated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a single message to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that!\n",
      "\n",
      "A flat white is a type of coffee drink that originated in Australia and New Zealand. It's a double shot of espresso topped with a thin layer of microfoam (steamed milk that's been frothed to a consistency similar to whipped cream). The ratio of espresso to milk is typically around 1:3, which means the drink is strong on coffee flavor but still has a creamy texture.\n",
      "\n",
      "Now, let's compare it to other popular coffee drinks like cappuccino:\n",
      "\n",
      "* Cappuccino: A traditional Italian drink that consists of one-third espresso, one-third steamed milk, and one-third frothed milk. The key difference is the layering of the ingredients, with the frothed milk on top in a cappuccino, whereas in a flat white, the microfoam is more evenly distributed throughout the drink.\n",
      "* Latte: A latte has a higher milk-to-coffee ratio than a flat white,\n"
     ]
    }
   ],
   "source": [
    "client = Llama3Client()\n",
    "\n",
    "message_text = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find answers. You use the Llama 3 8B model and can answer complex questions.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Tell me what flat white is and how it differs from other popular coffee beverages like cappuchino?\"},\n",
    "]\n",
    "\n",
    "completion = client.chat_completion(message_text, max_tokens=200)\n",
    "\n",
    "print(completion[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming responses\n",
    "\n",
    "Streaming is useful for longer responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutsche Telekom MMS, a subsidiary of Deutsche Telekom AG, is an Information Technology (IT) service provider offering managed services, cloud solutions, and business applications to customers. To understand what sets them apart, let's first consider the market landscape.\n",
      "\n",
      "In the IT services sector, companies typically focus on one or more areas:\n",
      "\n",
      "1. **Managed Services**: Proactively monitoring and managing customers' IT infrastructure, networks, and systems.\n",
      "2. **Cloud Services**: Offering cloud-based solutions for storage, computing, and applications, often with a subscription-based model.\n",
      "3. **Business Applications**: Providing software solutions for specific industries or business functions, such as enterprise resource planning (ERP) or customer relationship management (CRM).\n",
      "\n",
      "Deutsche Telekom MMS differentiates itself from other IT service providers in several ways:\n",
      "\n",
      "1. **Telecommunications heritage**: As a subsidiary of Deutsche Telekom AG, MMS benefits from the parent company's extensive telecommunications expertise and network infrastructure. This allows for seamless integration with customers"
     ]
    }
   ],
   "source": [
    "stream_resp = client.chat_completion(message_text, max_tokens=200, stream=True)\n",
    "\n",
    "full_response = \"\"\n",
    "for line in stream_resp:\n",
    "    if line:\n",
    "        chunk = json.loads(line)\n",
    "        if chunk.get(\"message\") and chunk[\"message\"].get(\"content\"):\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            full_response += content\n",
    "            print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs - Extracting JSON\n",
    "\n",
    "Llama 3 8B has improved structured reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "# Function to extract structured data with Llama 3 8B\n",
    "def extract_structured_data(prompt, schema_class, temperature=0.2):\n",
    "    # Lower temperature for more precise structured outputs\n",
    "    schema_dict = schema_class.model_json_schema()\n",
    "    schema_str = json.dumps(schema_dict, indent=2)\n",
    "    \n",
    "    # Optimized prompt for Llama 3 8B\n",
    "    structured_prompt = f\"\"\"\n",
    "    Extract the following information from the text and return it as valid JSON.\n",
    "    Schema: {schema_str}\n",
    "    \n",
    "    Text: {prompt}\n",
    "    \n",
    "    Respond with a valid JSON object (without Markdown formatting or additional text).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request with optimal parameters for structured extraction\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise assistant that extracts information as JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": structured_prompt},\n",
    "    ]\n",
    "    \n",
    "    response = client.chat_completion(messages, temperature=temperature)\n",
    "    \n",
    "    # Extract JSON from the response\n",
    "    response_text = response[\"message\"][\"content\"]\n",
    "    \n",
    "    # If the response contains Markdown code blocks\n",
    "    if \"```json\" in response_text:\n",
    "        json_str = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in response_text:\n",
    "        json_str = response_text.split(\"```\")[1].strip()\n",
    "    else:\n",
    "        json_str = response_text.strip()\n",
    "    \n",
    "    # Remove any comments if present\n",
    "    json_lines = [line for line in json_str.split('\\n') if not line.strip().startswith('//')]\n",
    "    clean_json = '\\n'.join(json_lines)\n",
    "    \n",
    "    # Convert JSON to a Python object\n",
    "    try:\n",
    "        result = json.loads(clean_json)\n",
    "        # Convert to the Pydantic model\n",
    "        return schema_class(**result)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Received JSON: {clean_json}\")\n",
    "        raise\n",
    "\n",
    "# Example with more complex text for Llama 3 8B\n",
    "event_text = \"\"\"Tina Smith and Dr. Tony Miller have scheduled to meet for the annual lottery \n",
    "at the community center on December 12, 2024. The event starts at 3:00 PM and prizes will be raffled off.\"\"\"\n",
    "\n",
    "event = extract_structured_data(event_text, CalendarEvent)\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation with Llama 3 8B\n",
    "\n",
    "Llama 3 8B has good conversational memory. Here's an example of a multi-turn dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for multi-turn dialogue\n",
    "def conversation(messages, print_response=True):\n",
    "    response = client.chat_completion(messages)\n",
    "    assistant_message = response[\"message\"]\n",
    "    \n",
    "    if print_response:\n",
    "        print(f\"Assistant: {assistant_message['content']}\\n\")\n",
    "    \n",
    "    # Add message to the dialogue\n",
    "    messages.append(assistant_message)\n",
    "    return messages\n",
    "\n",
    "# Start a new conversation\n",
    "convo = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who can explain complex topics clearly.\"}\n",
    "]\n",
    "\n",
    "# First question\n",
    "convo.append({\"role\": \"user\", \"content\": \"What are the key differences between traditional neural networks and transformer models?\"})\n",
    "convo = conversation(convo)\n",
    "\n",
    "# Follow-up question\n",
    "convo.append({\"role\": \"user\", \"content\": \"Can you give me an example where these differences matter in practice?\"})\n",
    "convo = conversation(convo)\n",
    "\n",
    "# Another follow-up\n",
    "convo.append({\"role\": \"user\", \"content\": \"Which programming languages and libraries should I learn to work with these models?\"})\n",
    "convo = conversation(convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Application: Document Summarization\n",
    "\n",
    "Llama 3 8B is good at summarizing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(text, max_length=200, focus_points=None):\n",
    "    \"\"\"Summarize a longer text with Llama 3 8B\"\"\"\n",
    "    \n",
    "    # Estimate if the text fits within context\n",
    "    estimated_tokens = len(text) // 4  # Rough estimation\n",
    "    \n",
    "    instructions = f\"Summarize the following text in about {max_length} words.\"\n",
    "    \n",
    "    if focus_points:\n",
    "        focus_str = \", \".join(focus_points)\n",
    "        instructions += f\" Focus particularly on these aspects: {focus_str}.\"\n",
    "    \n",
    "    if estimated_tokens > 3500:\n",
    "        print(\"Warning: The text might be too long for the context window. The summary might be incomplete.\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that creates precise summaries, capturing the key points.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{instructions}\\n\\nDOCUMENT: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    # Optimized parameters for summaries\n",
    "    response = client.chat_completion(messages, temperature=0.3, top_p=0.95)\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Example: Summarize a longer text\n",
    "sample_text = \"\"\"\n",
    "[This would be a longer text...]\n",
    "Modern software development faces numerous challenges. Agile methods have become the standard, \n",
    "but their implementation varies greatly depending on company culture and project requirements. DevOps practices enable \n",
    "faster development cycles, but require close collaboration between development and operations teams...\n",
    "[etc. for a longer text]\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_document(sample_text, focus_points=[\"Agile methods\", \"DevOps\", \"Collaboration\"])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Custom Llama 3 8B Model\n",
    "\n",
    "Here's how to create and use a custom version of Llama 3 8B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a Modelfile for a custom Llama 3 8B\n",
    "# You would run this in a terminal, not directly in the notebook\n",
    "\n",
    "'''\n",
    "# In terminal, create a file named Modelfile with these contents:\n",
    "\n",
    "FROM llama3:8b\n",
    "\n",
    "# Set model parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "# Define system behavior\n",
    "SYSTEM \"\"\"\n",
    "You are a helpful assistant who provides clear, accurate, and concise answers.\n",
    "You prefer structured, well-organized responses and list important information in a clear format.\n",
    "\"\"\"\n",
    "\n",
    "# Then build it with:\n",
    "ollama create custom-llama3-8b -f Modelfile\n",
    "'''\n",
    "\n",
    "print(\"Run the above commands in your terminal to create a custom Llama 3 8B model.\")\n",
    "print(\"After creating it, you can use it by changing MODEL_NAME to 'custom-llama3-8b' at the top of this notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
