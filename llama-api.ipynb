{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various interactions with local Llama3 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3:8b\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running with the following models:\n",
      " - german-mixtral:latest\n",
      " - mixtral:latest\n",
      " - llama3:8b\n",
      " - mannix/llama3.1-8b-abliterated:latest\n",
      " - llama3.1:latest\n",
      " - dolphin-llama3:latest\n",
      "\n",
      "✓ Llama 3 8B model found\n"
     ]
    }
   ],
   "source": [
    "def check_ollama_status():\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_ENDPOINT}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"models\", [])\n",
    "            if models:\n",
    "                print(f\"Ollama is running with the following models:\")\n",
    "                for model in models:\n",
    "                    print(f\" - {model['name']}\")\n",
    "                \n",
    "                # Check if Llama 3 8B is available\n",
    "                llama_models = [m for m in models if 'llama3:8b' in m['name'].lower()]\n",
    "                if llama_models:\n",
    "                    print(f\"\\n✓ Llama 3 8B model found\")\n",
    "                else:\n",
    "                    print(f\"\\n⚠ Llama 3 8B was not found\")\n",
    "            else:\n",
    "                print(\"Ollama is running, but no models were found. Run 'ollama pull llama3:8b' in the terminal.\")\n",
    "        else:\n",
    "            print(\"Could not connect to Ollama.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        print(\"Make sure the Ollama service is running with: 'systemctl --user start ollama'\")\n",
    "\n",
    "check_ollama_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3Client:\n",
    "    def __init__(self, base_url: str = OLLAMA_ENDPOINT, model: str = MODEL_NAME):\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        \n",
    "        # Default parameters optimized for llama3\n",
    "        self.default_params = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40,\n",
    "            \"num_ctx\": 4096  \n",
    "        }\n",
    "        \n",
    "    def chat_completion(self, messages: List[Dict[str, str]], \n",
    "                         max_tokens: Optional[int] = None, \n",
    "                         stream: bool = False,\n",
    "                         **kwargs):\n",
    "        endpoint = f\"{self.base_url}/api/chat\"\n",
    "        \n",
    "        # For inference\n",
    "        options = self.default_params.copy()\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            options[key] = value\n",
    "            \n",
    "        if max_tokens:\n",
    "            options[\"num_predict\"] = max_tokens\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "            \"options\": options\n",
    "        }\n",
    "            \n",
    "        if stream:\n",
    "            response = requests.post(endpoint, json=payload, stream=True)\n",
    "            return response.iter_lines()\n",
    "        else:\n",
    "            response = requests.post(endpoint, json=payload)\n",
    "            return response.json()\n",
    "    \n",
    "    def get_token_count(self, messages: List[Dict[str, str]]):\n",
    "        \"\"\"Estimate the number of tokens in the messages\"\"\"\n",
    "        # ~4 chars/token for llama3\n",
    "        total_chars = sum(len(m[\"content\"]) for m in messages)\n",
    "        estimated_tokens = total_chars // 4\n",
    "        return estimated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send a single message to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A delightful question!\n",
      "\n",
      "A flat white is a type of coffee drink that originated in Australia and New Zealand. It's a velvety-smooth, medium-to-strong coffee beverage made with espresso, microfoam (steamed milk that's been frothed to a consistency similar to whipped cream), and a thin layer of foam on top.\n",
      "\n",
      "Now, let's compare it to other popular coffee drinks:\n",
      "\n",
      "1. Cappuccino: A cappuccino is similar to a flat white in that it also contains espresso, steamed milk, and foam. However, the key difference lies in the ratio of these components:\n",
      "\t* Cappuccino: 1/3 espresso, 1/3 steamed milk, and 1/3 foam\n",
      "\t* Flat White: 2/3 espresso to 1/3 microfoam (no distinct layers)\n",
      "\n",
      "In a cappuccino, you'll typically find a clear distinction between the three components, whereas in a flat white\n"
     ]
    }
   ],
   "source": [
    "client = Llama3Client()\n",
    "\n",
    "message_text = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find answers. You use the Llama 3 8B model and can answer complex questions.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Tell me what flat white is and how it differs from other popular coffee beverages like cappuchino?\"},\n",
    "]\n",
    "\n",
    "completion = client.chat_completion(message_text, max_tokens=200)\n",
    "\n",
    "print(completion[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream responses\n",
    "Better for longer conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question!\n",
      "\n",
      "A flat white is a type of coffee drink that originated in Australia and New Zealand, characterized by its smooth, velvety texture and rich flavor profile. It's made with espresso and steamed milk, but what sets it apart from other popular coffee drinks like cappuccino is the ratio of espresso to milk.\n",
      "\n",
      "In a flat white, the ratio is typically 1:3 or 1:4 (one part espresso to three or four parts milk), which means that the drink is more milky than espresso-forward. The steamed milk is also heated to a lower temperature than in a cappuccino, resulting in a silky texture rather than a thick, foamy one.\n",
      "\n",
      "In contrast, a traditional cappuccino typically has a 1:5 or 1:6 ratio of espresso to milk, with a thicker layer of foam on top. This means that the drink is more espresso-forward and has a stronger, more intense flavor profile.\n",
      "\n",
      "Other key"
     ]
    }
   ],
   "source": [
    "stream_resp = client.chat_completion(message_text, max_tokens=200, stream=True)\n",
    "\n",
    "full_response = \"\"\n",
    "for line in stream_resp:\n",
    "    if line:\n",
    "        chunk = json.loads(line)\n",
    "        if chunk.get(\"message\") and chunk[\"message\"].get(\"content\"):\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            full_response += content\n",
    "            print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Traditional neural networks, also known as feedforward neural networks or recurrent neural networks (RNNs), have been the backbone of deep learning for many years. However, Transformer models, introduced in 2017 by Vaswani et al., have revolutionized the field of natural language processing and beyond.\n",
      "\n",
      "Here are the key differences between traditional neural networks and Transformer models:\n",
      "\n",
      "1. **Sequential vs. Parallel Processing**: Traditional RNNs process input sequences sequentially, one element at a time. Transformers, on the other hand, process input sequences in parallel using self-attention mechanisms.\n",
      "2. **Attention Mechanism**: Transformers use multi-head self-attention to weigh the importance of different parts of the input sequence relative to each other and the model's internal state. This allows for more flexible and context-aware processing. Traditional RNNs rely on recurrence or convolutional layers to capture long-range dependencies.\n",
      "3. **Encoder-Decoder Architecture**: Transformer models consist of an encoder and a decoder. The encoder maps the input sequence to a continuous representation, while the decoder generates the output sequence. In traditional RNNs, the encoder-decoder architecture is not as prominent.\n",
      "4. **No Recurrence or Convolution**: Transformers do not use recurrence (LSTMs, GRUs) or convolutional layers. Instead, they rely on self-attention and feed-forward neural networks (FFNNs) to process input sequences.\n",
      "5. **Scalability**: Transformer models can handle much longer input sequences than traditional RNNs due to their parallel processing nature. This makes them particularly well-suited for tasks like machine translation, text summarization, and language generation.\n",
      "6. **Computational Complexity**: Transformers require more computational resources than traditional RNNs, especially for large input sequences. However, this complexity is often justified by the improved performance and efficiency of Transformer models.\n",
      "7. **Training Objectives**: Traditional RNNs are often trained using maximum likelihood estimation (MLE) or variants like masked language modeling (MLM). Transformers, on the other hand, are typically trained using MLE or MLM, but also employ techniques like teacher forcing and parallel computing to accelerate training.\n",
      "8. **Model Size and Capacity**: Transformer models tend to be larger than traditional RNNs, requiring more parameters to capture complex relationships in input sequences.\n",
      "\n",
      "In summary, Transformer models have introduced significant innovations in the field of deep learning, offering improved performance, scalability, and flexibility for tasks involving sequential data processing. While traditional neural networks are still useful for certain applications, Transformers have become a popular choice for many natural language processing and machine learning tasks.\n",
      "\n",
      "Assistant: Let's consider the task of machine translation from English to French. Traditional RNN-based models like seq2seq (sequence-to-sequence) or NMT (Neural Machine Translation) have been used for this task, but they often struggle with:\n",
      "\n",
      "1. **Sequential processing**: RNNs process input sequences sequentially, which can lead to a \"source-target\" mismatch. For example, when translating \"What's your name?\" to French, the model might focus too much on the verb \"what\" instead of considering the entire sentence.\n",
      "2. ** Limited context awareness**: RNNs rely heavily on recurrence or convolutional layers to capture long-range dependencies. This can lead to a lack of context-awareness, making it difficult for the model to understand complex sentences.\n",
      "\n",
      "Now, let's consider a Transformer-based machine translation model. With its parallel processing and self-attention mechanisms, it can:\n",
      "\n",
      "1. **Process input sequences in parallel**: The Transformer model can process the entire input sentence simultaneously, allowing it to capture complex relationships between words.\n",
      "2. **Capture long-range dependencies**: Self-attention enables the model to weigh the importance of different parts of the input sequence relative to each other and the internal state. This allows for more accurate translation of sentences with long-range dependencies.\n",
      "\n",
      "In practice, this means that a Transformer-based machine translation model can:\n",
      "\n",
      "* Handle longer input sequences without degrading performance\n",
      "* Translate complex sentences more accurately by capturing long-range dependencies and contextual relationships\n",
      "* Improve overall translation quality and fluency\n",
      "\n",
      "For example, if you ask a traditional RNN-based model to translate \"I ate breakfast at 8 am yesterday\" to French, it might struggle with the verb tenses and sentence structure. A Transformer-based model, on the other hand, can capture the context of the entire sentence and produce a more accurate translation like \"J'ai fait des petit déjeuner à 8 heures hier.\"\n",
      "\n",
      "In this example, the differences between traditional RNNs and Transformers matter in practice because they impact the ability to handle complex input sequences and long-range dependencies.\n",
      "\n",
      "Assistant: To work with Transformer models and other deep learning architectures, you'll need to familiarize yourself with popular programming languages and libraries for machine learning and natural language processing (NLP). Here are some essential ones:\n",
      "\n",
      "1. **Python**: Python is the most widely used language for machine learning and NLP. You should learn Python as your primary language.\n",
      "2. **TensorFlow** or **PyTorch**: Both TensorFlow and PyTorch are popular open-source libraries for building and training deep learning models, including Transformers. TensorFlow is more mature and widely used, while PyTorch is known for its ease of use and dynamic computation graphs.\n",
      "3. **Keras**: Keras is a high-level neural networks API that can run on top of TensorFlow or Theano. It's a great library for building and experimenting with deep learning models, including Transformers.\n",
      "\n",
      "Additionally, you might want to learn:\n",
      "\n",
      "1. **NLTK** (Natural Language Toolkit) or **spaCy**: These libraries provide pre-trained models and tools for processing natural language data, such as tokenization, part-of-speech tagging, and named entity recognition.\n",
      "2. **OpenNLP**: OpenNLP is an open-source library for maximum entropy-based NLP tasks like text classification, named entity recognition, and sentence parsing.\n",
      "3. ** scikit-learn**: Scikit-learn is a machine learning library that provides tools for data preprocessing, feature selection, and model selection.\n",
      "\n",
      "To get started with these libraries, I recommend the following resources:\n",
      "\n",
      "* Python: Codecademy's Python course or Google's Python tutorial\n",
      "* TensorFlow: TensorFlow's official tutorials and documentation\n",
      "* PyTorch: PyTorch's official tutorials and documentation\n",
      "* Keras: Keras' official tutorials and documentation\n",
      "\n",
      "For NLP-specific topics, you can explore:\n",
      "\n",
      "* Stanford Natural Language Processing Group's online courses (e.g., Natural Language Processing with Deep Learning)\n",
      "* Coursera's Machine Learning Specialization by Andrew Ng (includes NLP topics)\n",
      "* edX's Natural Language Processing course by MIT\n",
      "\n",
      "Remember to practice and build projects as you learn. This will help you solidify your understanding of the concepts and libraries. Good luck!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def conversation(messages, print_response=True):\n",
    "    response = client.chat_completion(messages)\n",
    "    assistant_message = response[\"message\"]\n",
    "    \n",
    "    if print_response:\n",
    "        print(f\"Assistant: {assistant_message['content']}\\n\")\n",
    "    \n",
    "    # Add message to the dialogue\n",
    "    messages.append(assistant_message)\n",
    "    return messages\n",
    "\n",
    "# Start new conversation\n",
    "convo = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who can explain complex topics clearly.\"}\n",
    "]\n",
    "\n",
    "convo.append({\"role\": \"user\", \"content\": \"What are the key differences between traditional neural networks and transformer models?\"})\n",
    "convo = conversation(convo)\n",
    "\n",
    "convo.append({\"role\": \"user\", \"content\": \"Can you give me an example where these differences matter in practice?\"})\n",
    "convo = conversation(convo)\n",
    "\n",
    "convo.append({\"role\": \"user\", \"content\": \"Which programming languages and libraries should I learn to work with these models?\"})\n",
    "convo = conversation(convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the text, focusing on animals and research:\n",
      "\n",
      "The axolotl, a salamander closely related to the tiger salamander, is an unusual amphibian that reaches adulthood without undergoing metamorphosis. It remains aquatic and gilled as an adult. Originally found in lakes near Mexico City, the species was nearly wiped out due to habitat destruction and urbanization. As of 2020, the axolotl population had declined to around 50-1,000 individuals, making it critically endangered.\n",
      "\n",
      "Axolotls are highly valued for their ability to regenerate limbs, gills, eyes, and brains, making them a popular subject in scientific research. Their regenerative abilities decline with age, but do not disappear entirely. Researchers have also studied the axolotl's heart as a model for human single ventricle and excessive trabeculation.\n",
      "\n",
      "Overall, the axolotl is an important species in both its natural habitat and in the realm of scientific inquiry.\n"
     ]
    }
   ],
   "source": [
    "def summarize_document(text, max_length=200, focus_points=None):\n",
    "    \n",
    "    # Estimate if text fits within context ~4 char/token\n",
    "    estimated_tokens = len(text) // 4  \n",
    "    \n",
    "    instructions = f\"Summarize the following text in about {max_length} words.\"\n",
    "    \n",
    "    if focus_points:\n",
    "        focus_str = \", \".join(focus_points)\n",
    "        instructions += f\" Focus particularly on these aspects: {focus_str}.\"\n",
    "    \n",
    "    if estimated_tokens > 3500:\n",
    "        print(\"Warning: The text might be too long for the context window. The summary might be incomplete.\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that creates precise summaries, capturing the key points.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{instructions}\\n\\nDOCUMENT: {text}\"}\n",
    "    ]\n",
    "    \n",
    "    # Optimized parameters for summaries\n",
    "    response = client.chat_completion(messages, temperature=0.3, top_p=0.95)\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Example: Summarize a longer text\n",
    "sample_text = \"\"\"\n",
    "The axolotl  is a paedomorphic salamander closely related to the tiger salamander.[3][4][5] It is unusual among amphibians in that it reaches adulthood without undergoing metamorphosis. Instead of taking to the land, adults remain aquatic and gilled. The species was originally found in several lakes underlying what is now Mexico City, such as Lake Xochimilco and Lake Chalco.[1] These lakes were drained by Spanish settlers after the conquest of the Aztec Empire, leading to the destruction of much of the axolotl's natural habitat.\n",
    "\n",
    "As of 2020, the axolotl was near extinction[6][7] due to urbanization in Mexico City and consequent water pollution, as well as the introduction of invasive species such as tilapia and perch. It is listed as critically endangered in the wild, with a decreasing population of around 50 to 1,000 adult individuals, by the International Union for Conservation of Nature and Natural Resources (IUCN) and is listed under Appendix II of the Convention on International Trade in Endangered Species (CITES).[2] Axolotls are used extensively in scientific research for their ability to regenerate limbs, gills and parts of their eyes and brains.[8] Notably, their ability to regenerate declines with age, but it does not disappear. Axolotls keep modestly growing throughout their life and some consider this trait to be a direct contributor to their regenerative abilities.[9] Further research has been conducted to examine their heart as a model of human single ventricle and excessive trabeculation.[10] Axolotls were also sold as food in Mexican markets and were a staple in the Aztec diet.[11]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_document(sample_text, focus_points=[\"Animals\", \"Research\"])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract structured data\n",
    "Really useful for tooling \n",
    "##### TODO work on realiability of parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean JSON to parse: {\n",
      "      \"name\": \"annual Kaffekranz\",\n",
      "      \"date\": \"December 31, 2042\",\n",
      "      \"participants\": [\"Rick\", \"Morty\"]\n",
      "    }\n",
      "name='annual Kaffekranz' date='December 31, 2042' participants=['Rick', 'Morty']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "def extract_structured_data(prompt, schema_class, temperature=0.2):\n",
    "    # Lower temperature --> more precise structured outputs\n",
    "    schema_dict = schema_class.model_json_schema()\n",
    "    schema_str = json.dumps(schema_dict, indent=2)\n",
    "    \n",
    "    structured_prompt = f\"\"\"\n",
    "    Extract the following information from the text and return it as valid JSON.\n",
    "    Schema: {schema_str}\n",
    "    \n",
    "    Text: {prompt}\n",
    "    \n",
    "    Respond with a valid JSON object (without Markdown formatting or additional text).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request with optimal parameters for structured extraction\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise assistant that extracts information as JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": structured_prompt},\n",
    "    ]\n",
    "    \n",
    "    # Send request to API\n",
    "    # response = client.chat_completion(messages, temperature=temperature)\n",
    "    # response_text = response[\"message\"][\"content\"]\n",
    "    \n",
    "    # For debugging - simulate a response for the given input\n",
    "    response_text = \"\"\"```json\n",
    "    {\n",
    "      \"name\": \"annual Kaffekranz\",\n",
    "      \"date\": \"December 31, 2042\",\n",
    "      \"participants\": [\"Rick\", \"Morty\"]\n",
    "    }\n",
    "    ```\"\"\"\n",
    "    \n",
    "    # CLEANUP response data\n",
    "    clean_json = \"\"\n",
    "    \n",
    "    # Check for code blocks\n",
    "    if \"```json\" in response_text:\n",
    "        # Extract content between ```json and ```\n",
    "        match = re.search(r\"```json\\n(.*?)```\", response_text, re.DOTALL)\n",
    "        if match:\n",
    "            clean_json = match.group(1).strip()\n",
    "    elif \"```\" in response_text:\n",
    "        # Extract content between ``` and ```\n",
    "        match = re.search(r\"```\\n?(.*?)```\", response_text, re.DOTALL)\n",
    "        if match:\n",
    "            clean_json = match.group(1).strip()\n",
    "    else:\n",
    "        # No code blocks, use the entire response\n",
    "        clean_json = response_text.strip()\n",
    "    \n",
    "    # Remove any comments\n",
    "    json_lines = [line for line in clean_json.split('\\n') if not line.strip().startswith('//')]\n",
    "    clean_json = '\\n'.join(json_lines)\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"Clean JSON to parse: {clean_json}\")\n",
    "    \n",
    "    # Convert JSON to a Python object\n",
    "    try:\n",
    "        result = json.loads(clean_json)\n",
    "        # Convert to the Pydantic model\n",
    "        return schema_class(**result)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Received JSON: {clean_json!r}\")  # Use repr to see hidden characters\n",
    "        \n",
    "        # Additional fallback for common issues\n",
    "        if clean_json:\n",
    "            # Try to fix common JSON formatting issues\n",
    "            try:\n",
    "                # Replace single quotes with double quotes\n",
    "                fixed_json = clean_json.replace(\"'\", \"\\\"\")\n",
    "                # Try parsing again\n",
    "                result = json.loads(fixed_json)\n",
    "                return schema_class(**result)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        raise\n",
    "\n",
    "event_text = \"\"\"Rick and Morty are scheduled to meet for the annual Kaffekranz\n",
    "at Milliways on December 31, 2042. Cakes and cats are included.\"\"\"\n",
    "\n",
    "event = extract_structured_data(event_text, CalendarEvent)\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelfile Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFROM llama3:8b\\n\\n# Set model parameters\\nPARAMETER temperature 0.7\\nPARAMETER top_p 0.9\\nPARAMETER top_k 40\\nPARAMETER num_ctx 4096\\n\\n# Define system behavior\\nSYSTEM \"\"\"\\nYou are a helpful assistant who provides clear, accurate, and concise answers.\\nYou prefer structured, well-organized responses and list important information in a clear format.\\n\"\"\"\\n\\n# Then build it with:\\nollama create custom-llama3-8b -f Modelfile\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this in a terminal, not directly in the notebook\n",
    "\n",
    "'''touch MODELFILE '''\n",
    "\n",
    "'''\n",
    "\n",
    "FROM llama3:8b\n",
    "\n",
    "# Set model parameters\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "# Define system behavior\n",
    "SYSTEM \"\"\"\n",
    "You are a helpful assistant who provides clear, accurate, and concise answers.\n",
    "You prefer structured, well-organized responses and list important information in a clear format.\n",
    "\"\"\"\n",
    "\n",
    "# Then build it with:\n",
    "ollama create custom-llama3-8b -f Modelfile\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
