{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da81fc8",
   "metadata": {},
   "source": [
    "# Raw basics\n",
    "Some under-the-hood for building intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f43447",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305d19c",
   "metadata": {},
   "source": [
    "##### Basic Neuron\n",
    "\n",
    "              ┌─────────────────────────────────────────────┐\n",
    "              │               Neuron                        │                                              \n",
    "              │                                             │\n",
    "  Inputs      │     Weights         Computation             │    Output\n",
    "              │                                             │\n",
    "   x₁ = 0.5 ──┼──>  w₁ = 0.2 ──┐                            │\n",
    "              │                │                            │\n",
    "   x₂ = 1.0 ──┼──>  w₂ = 0.8 ──┼─► z = Σ(wᵢxᵢ) + b          │\n",
    "              │                │    = 0.2×0.5 + 0.8×1.0     │\n",
    "   x₃ = 0.3 ──┼──>  w₃ = -0.1 ─┤    + (-0.1)×0.3 + 0.5      │\n",
    "              │                │    = 0.1 + 0.8 - 0.03 + 0.5│       ┌─────┐\n",
    "              │                │    = 1.37                  ├───►   │ 1   │  = 0.798\n",
    "              │                │                            │       │─────│\n",
    "              │     bias = 0.5 ┘                            │       │1+e⁻ᶻ│\n",
    "              │                                             │       └─────┘\n",
    "              └─────────────────────────────────────────────┘       Activation Function \n",
    "                                                                    \n",
    "Sigmoid Activation Function\n",
    "                                                                      \n",
    "  1 │       ---------------------\n",
    "    │      /              │\n",
    "    │     /               │\n",
    "    │    /                │\n",
    "y   │   /                 │\n",
    "    │  /                  │\n",
    "    │ /                   │\n",
    "  0 │/                    │\n",
    "    └---------------------|------->\n",
    "    -6      0      z      6\n",
    "           Input value\n",
    "\n",
    "A Neuron..\n",
    "* Takes multiple inputs (x₁, x₂, x₃, ...)\n",
    "* Multiplies each by its corresponding weight (w₁, w₂, w₃, ...)\n",
    "* Sums these products + bias value\n",
    "* Passes this sum through an activation function (sigmoid)\n",
    "* Out falls a single output value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Single Neuron ---\n",
      "Neuron structure: weights=[0.5, -0.5], bias=0.1\n",
      "Input: [0, 0], Output: 0.5250\n",
      "Input: [1, 0], Output: 0.6457\n",
      "Input: [0, 1], Output: 0.4013\n",
      "Input: [1, 1], Output: 0.5250\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        # Initialize weights with small random values (-1 to 1)\n",
    "        # Each neuron has one weight per input connection\n",
    "        self.weights = [random.uniform(-1, 1) for _ in range(num_inputs)]\n",
    "        self.bias = random.uniform(-1, 1)\n",
    "    \n",
    "    def activate(self, inputs):\n",
    "        # Calculate weighted sum of inputs plus bias\n",
    "        # MATH: z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias\n",
    "        return self.sigmoid(weighted_sum)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function - maps any input to range (0, 1)\n",
    "        # Helps introduce non-linearity into the model\n",
    "        # MATH: σ(x) = 1 / (1 + e^(-x))\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38521748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Single Neuron with two inputs ---\n",
      "Neuron structure: weights=[0.18257723684684413, -0.9882815359352428], bias=0.9031078190837833\n",
      "Input: [0, 0], Output: 0.7116\n",
      "Input: [1, 0], Output: 0.7476\n",
      "Input: [0, 1], Output: 0.4787\n",
      "Input: [1, 1], Output: 0.5243\n",
      "\n",
      "--- Testing Single Neuron with three inputs ---\n",
      "Input: [0.5, 0.2, 0.9], Output: 0.4664\n",
      "Input: [0.1, 0.3, 0.7], Output: 0.4887\n",
      "Input: [0.8, 0.4, 0.2], Output: 0.2755\n"
     ]
    }
   ],
   "source": [
    "test_neuron = Neuron(1)  # Create a neuron with ONE input\n",
    "test_inputs = [0.5, 0.2, 0.9]  # Different test cases\n",
    "for input_value in test_inputs:\n",
    "    # Pass as list with single value\n",
    "    output = test_neuron.activate([input_value])\n",
    "    print(f\"Input: {input_value}, Output: {output:.4f}\")\n",
    "\n",
    "# Neuron with two inputs\n",
    "print(\"\\n--- Testing Single Neuron with two inputs ---\")\n",
    "test_neuron = Neuron(2)\n",
    "    \n",
    "print(f\"Neuron structure: weights={test_neuron.weights}, bias={test_neuron.bias}\")\n",
    "test_inputs = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "for inputs in test_inputs:\n",
    "    output = test_neuron.activate(inputs)\n",
    "    print(f\"Input: {inputs}, Output: {output:.4f}\")\n",
    "\n",
    "# Neuron with three inputs   \n",
    "print(\"\\n--- Testing Single Neuron with three inputs ---\")\n",
    "test_neuron = Neuron(3) \n",
    "# Create test cases - each is a list of 3 values\n",
    "test_cases = [\n",
    "    [0.5, 0.2, 0.9],\n",
    "    [0.1, 0.3, 0.7],\n",
    "    [0.8, 0.4, 0.2]\n",
    "]\n",
    "for inputs in test_cases:\n",
    "    output = test_neuron.activate(inputs)\n",
    "    print(f\"Input: {inputs}, Output: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b164e1",
   "metadata": {},
   "source": [
    "### Basic Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, num_neurons, num_inputs_per_neuron):\n",
    "        # Create a layer with multiple neurons, all receiving the same number of inputs\n",
    "        self.neurons = [Neuron(num_inputs_per_neuron) for _ in range(num_neurons)]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Calculate output for each neuron in the layer given the same inputs\n",
    "        # Returns a list of outputs, one per neuron\n",
    "        outputs = [neuron.activate(inputs) for neuron in self.neurons]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0d0d6",
   "metadata": {},
   "source": [
    "### Neural Network Part 1\n",
    "Lets look at the whole learning process. The name for this is Backpropagation and its named after the way information flows trough a neural network.\n",
    "\n",
    "Information Flow:\n",
    "Forward Pass --> Error Calculation --> Output Layer Backpropagation --> Hidden Layer Backpropagation --> Weight and Bias Updates --> REPEAT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        # input2hidden layer\n",
    "        self.hidden_layer = Layer(num_hidden, num_inputs)\n",
    "        # hidden2output layer\n",
    "        self.output_layer = Layer(num_outputs, num_hidden)\n",
    "\n",
    "    def forward (self, inputs):\n",
    "        # Pass inputs through hidden layer       \n",
    "        # MATH h_j = σ(Σ w_ji * x_i + b_j)  ...where j is hidden neuron index, i is input index\n",
    "        hidden_outputs = self.hidden_layer.forward(inputs)\n",
    "        # Pass hidden layer outputs to output layer\n",
    "        # MATH: #  o_k = σ(Σ w_kj * h_j + b_k) ...where k is output neuron index, j is hidden neuron index\n",
    "        return self.output_layer.forward(hidden_outputs)\n",
    "\n",
    "    def train():\n",
    "        \"\"\"\n",
    "        Train the neural network using backpropagation.\n",
    "        \n",
    "        Parameters:\n",
    "            training_data: List of (inputs, targets) tuples\n",
    "            learning_rate: Controls how quickly the network learns (step size)\n",
    "            epochs: Number of complete passes through the training dataset\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea24ae0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e65b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
